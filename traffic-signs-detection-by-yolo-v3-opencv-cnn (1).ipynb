{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Signs Detection with YOLO v3, OpenCV and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì• Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import cv2\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÇ Loading *labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ClassId              SignName\n",
      "0        0  Speed limit (20km/h)\n",
      "1        1  Speed limit (30km/h)\n",
      "2        2  Speed limit (50km/h)\n",
      "3        3  Speed limit (60km/h)\n",
      "4        4  Speed limit (70km/h)\n",
      "\n",
      "Speed limit (20km/h)\n",
      "Speed limit (30km/h)\n"
     ]
    }
   ],
   "source": [
    "# Reading csv file with labels' names\n",
    "# Loading two columns [0, 1] into Pandas dataFrame\n",
    "labels = pd.read_csv('C:/Users/palash/Downloads/datasets_327959_657000_label_names.csv')\n",
    "\n",
    "# Check point\n",
    "# Showing first 5 rows from the dataFrame\n",
    "print(labels.head())\n",
    "print()\n",
    "\n",
    "print(labels.iloc[0][1])  # Speed limit (20km/h)\n",
    "# ***['SignName'][1] - returns element on the column with name 'SignName' and 1 row\n",
    "print(labels['SignName'][1]) # Speed limit (30km/h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìç Loading trained Keras CNN model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading trained CNN model to use it later when classifying from 4 groups into one of 43 classes\n",
    "model = load_model('C:/Users/palash/Downloads/model (1).h5')\n",
    "\n",
    "#with open('../input/traffic-signs-preprocessed/mean_image_rgb.pickle', 'rb') as f:\n",
    "    #mean = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "#print(mean['mean_image_rgb'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí† Loading YOLO v3 network by OpenCV dnn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading *trained weights* and *cfg file* into the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_weights = 'C:/yolov3_ts.weights'\n",
    "path_to_cfg = 'C:/Users/palash/Downloads/yolov3_ts_test (2).cfg'\n",
    "\n",
    "# Loading trained YOLO v3 weights and cfg configuration file by 'dnn' library from OpenCV\n",
    "network = cv2.dnn.readNetFromDarknet(path_to_cfg, path_to_weights)\n",
    "\n",
    "network.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "network.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting *output layers* where detections are made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['yolo_82', 'yolo_94', 'yolo_106']\n"
     ]
    }
   ],
   "source": [
    "layers_all = network.getLayerNames()\n",
    "# Getting only detection YOLO v3 layers that are 82, 94 and 106\n",
    "layers_names_output = [layers_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\n",
    "# Check point\n",
    "print()\n",
    "print(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting *probability*, *threshold* and *colour* for bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(43, 3)\n",
      "[168 249  73]\n"
     ]
    }
   ],
   "source": [
    "# Minimum probability to eliminate weak detections\n",
    "probability_minimum = 0.2\n",
    "\n",
    "# Setting threshold to filtering weak bounding boxes by non-maximum suppression\n",
    "threshold = 0.2\n",
    "\n",
    "# Generating colours for bounding boxes\n",
    "colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
    "\n",
    "print(type(colours))  \n",
    "print(colours.shape)  \n",
    "print(colours[0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Reading input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading video from a file by VideoCapture object\n",
    "video = cv2.VideoCapture('../input/traffic-signs-dataset-in-yolo-format/traffic-sign-to-test.mp4')\n",
    "\n",
    "# Writer that will be used to write processed frames\n",
    "writer = None\n",
    "\n",
    "# Variables for spatial dimensions of the frames\n",
    "h, w = None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing frames in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame number 1 took 1.39053 seconds\n",
      "Frame number 2 took 1.11543 seconds\n",
      "Frame number 3 took 1.09141 seconds\n",
      "Frame number 4 took 1.09869 seconds\n",
      "Frame number 5 took 1.10503 seconds\n",
      "Frame number 6 took 1.10214 seconds\n",
      "Frame number 7 took 1.12371 seconds\n",
      "Frame number 8 took 1.10542 seconds\n",
      "Frame number 9 took 1.11623 seconds\n",
      "Frame number 10 took 1.11995 seconds\n",
      "Frame number 11 took 1.10640 seconds\n",
      "Frame number 12 took 1.11359 seconds\n",
      "Frame number 13 took 1.13821 seconds\n",
      "Frame number 14 took 1.09765 seconds\n",
      "Frame number 15 took 1.09825 seconds\n",
      "Frame number 16 took 1.14865 seconds\n",
      "Frame number 17 took 1.24434 seconds\n",
      "Frame number 18 took 1.13595 seconds\n",
      "Frame number 19 took 1.09914 seconds\n",
      "Frame number 20 took 1.28085 seconds\n",
      "Frame number 21 took 1.24353 seconds\n",
      "Frame number 22 took 1.23312 seconds\n",
      "Frame number 23 took 1.13254 seconds\n",
      "Frame number 24 took 1.10308 seconds\n",
      "Frame number 25 took 1.19745 seconds\n",
      "Frame number 26 took 1.18168 seconds\n",
      "Frame number 27 took 1.13057 seconds\n",
      "Frame number 28 took 1.15409 seconds\n",
      "Frame number 29 took 1.14314 seconds\n",
      "Frame number 30 took 1.14345 seconds\n",
      "Frame number 31 took 1.09830 seconds\n",
      "Frame number 32 took 1.09723 seconds\n",
      "Frame number 33 took 1.16779 seconds\n",
      "Frame number 34 took 1.11551 seconds\n",
      "Frame number 35 took 1.09334 seconds\n",
      "Frame number 36 took 1.09426 seconds\n",
      "Frame number 37 took 1.09809 seconds\n",
      "Frame number 38 took 1.10073 seconds\n",
      "Frame number 39 took 1.10154 seconds\n",
      "Frame number 40 took 1.09540 seconds\n",
      "Frame number 41 took 1.23908 seconds\n",
      "Frame number 42 took 1.15313 seconds\n",
      "Frame number 43 took 1.09302 seconds\n",
      "Frame number 44 took 1.10127 seconds\n",
      "Frame number 45 took 1.11232 seconds\n",
      "Frame number 46 took 1.10714 seconds\n",
      "Frame number 47 took 1.10466 seconds\n",
      "Frame number 48 took 1.10101 seconds\n",
      "Frame number 49 took 1.11945 seconds\n",
      "Frame number 50 took 1.10250 seconds\n",
      "Frame number 51 took 1.15393 seconds\n",
      "Frame number 52 took 1.10599 seconds\n",
      "Frame number 53 took 1.10136 seconds\n",
      "Frame number 54 took 1.10024 seconds\n",
      "Frame number 55 took 1.09891 seconds\n",
      "Frame number 56 took 1.10041 seconds\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of plots\n",
    "plt.rcParams['figure.figsize'] = (3, 3)\n",
    "\n",
    "# Variable for counting total amount of frames\n",
    "f = 0\n",
    "\n",
    "# Variable for counting total processing time\n",
    "t = 0\n",
    "\n",
    "# Catching frames in the loop\n",
    "while True:\n",
    "    # Capturing frames one-by-one\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    # If the frame was not retrieved\n",
    "    if not ret:\n",
    "        break\n",
    "       \n",
    "    # Getting spatial dimensions of the frame for the first time\n",
    "    if w is None or h is None:\n",
    "        # Slicing two elements from tuple\n",
    "        h, w = frame.shape[:2]\n",
    "\n",
    "    # Blob from current frame\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "    # Forward pass with blob through output layers\n",
    "    network.setInput(blob)\n",
    "    start = time.time()\n",
    "    output_from_network = network.forward(layers_names_output)\n",
    "    end = time.time()\n",
    "\n",
    "    # Increasing counters\n",
    "    f += 1\n",
    "    t += end - start\n",
    "\n",
    "    # Spent time for current frame\n",
    "    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n",
    "\n",
    "    # Lists for detected bounding boxes, confidences and class's number\n",
    "    bounding_boxes = []\n",
    "    confidences = []\n",
    "    class_numbers = []\n",
    "\n",
    "    # Going through all output layers after feed forward pass\n",
    "    for result in output_from_network:\n",
    "        # Going through all detections from current output layer\n",
    "        for detected_objects in result:\n",
    "            # Getting 80 classes' probabilities for current detected object\n",
    "            scores = detected_objects[5:]\n",
    "            # Getting index of the class with the maximum value of probability\n",
    "            class_current = np.argmax(scores)\n",
    "            # Getting value of probability for defined class\n",
    "            confidence_current = scores[class_current]\n",
    "\n",
    "            # Eliminating weak predictions by minimum probability\n",
    "            if confidence_current > probability_minimum:\n",
    "                # Scaling bounding box coordinates to the initial frame size\n",
    "                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
    "\n",
    "                # Getting top left corner coordinates\n",
    "                x_center, y_center, box_width, box_height = box_current\n",
    "                x_min = int(x_center - (box_width / 2))\n",
    "                y_min = int(y_center - (box_height / 2))\n",
    "\n",
    "                # Adding results into prepared lists\n",
    "                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
    "                confidences.append(float(confidence_current))\n",
    "                class_numbers.append(class_current)\n",
    "                \n",
    "\n",
    "    # Implementing non-maximum suppression of given bounding boxes\n",
    "    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n",
    "\n",
    "    # Checking if there is any detected object been left\n",
    "    if len(results) > 0:\n",
    "        # Going through indexes of results\n",
    "        for i in results.flatten():\n",
    "            # Bounding box coordinates, its width and height\n",
    "            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
    "            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
    "            \n",
    "            \n",
    "            # Cut fragment with Traffic Sign\n",
    "            c_ts = frame[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n",
    "            # print(c_ts.shape)\n",
    "            \n",
    "            if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n",
    "                pass\n",
    "            else:\n",
    "                # Getting preprocessed blob with Traffic Sign of needed shape\n",
    "                blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n",
    "                blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n",
    "                blob_ts = blob_ts.transpose(0, 2, 3, 1)\n",
    "                # plt.imshow(blob_ts[0, :, :, :])\n",
    "                # plt.show()\n",
    "\n",
    "                # Feeding to the Keras CNN model to get predicted label among 43 classes\n",
    "                scores = model.predict(blob_ts)\n",
    "\n",
    "                # Scores is given for image with 43 numbers of predictions for each class\n",
    "                # Getting only one class with maximum value\n",
    "                prediction = np.argmax(scores)\n",
    "                # print(labels['SignName'][prediction])\n",
    "\n",
    "\n",
    "                # Colour for current bounding box\n",
    "                colour_box_current = colours[class_numbers[i]].tolist()\n",
    "\n",
    "                # Drawing bounding box on the original current frame\n",
    "                cv2.rectangle(frame, (x_min, y_min),\n",
    "                              (x_min + box_width, y_min + box_height),\n",
    "                              colour_box_current, 2)\n",
    "\n",
    "                # Preparing text with label and confidence for current bounding box\n",
    "                text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],\n",
    "                                                       confidences[i])\n",
    "\n",
    "                # Putting text with label and confidence on the original image\n",
    "                cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n",
    "\n",
    "\n",
    "    # Initializing writer only once\n",
    "    if writer is None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "        # Writing current processed frame into the video file\n",
    "        writer = cv2.VideoWriter('result.mp4', fourcc, 30,\n",
    "                                 (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    # Write processed current frame to the file\n",
    "    writer.write(frame)\n",
    "\n",
    "\n",
    "# Releasing video reader and writer\n",
    "video.release()\n",
    "writer.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ FPS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames 56\n",
      "Total amount of time 63.45080 seconds\n",
      "FPS: 0.9\n"
     ]
    }
   ],
   "source": [
    "print('Total number of frames', f)\n",
    "print('Total amount of time {:.5f} seconds'.format(t))\n",
    "print('FPS:', round((f / t), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='result.mp4' target='_blank'>result.mp4</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/result.mp4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving locally without committing\n",
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink('result.mp4')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
